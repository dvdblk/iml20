{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images / Data\n",
    "\n",
    "## Standardize\n",
    "\n",
    "* Change the size of each image to the median values of the entire image set (460 x 310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 10000\n"
     ]
    }
   ],
   "source": [
    "IMAGES_FILEPATH = 'data/food'\n",
    "PROCESSED_IMAGES_FILEPATH = 'data/food_processed'\n",
    "IMAGE_DIMS = (460, 310)\n",
    "\n",
    "def listdir_jpg(path):\n",
    "    \"\"\"os.listdir but only for jpg\"\"\"\n",
    "    for f in os.listdir(path):\n",
    "        _, extension = os.path.splitext(f)\n",
    "        if extension == \".jpg\":\n",
    "            yield f\n",
    "\n",
    "number_of_images = len(list(listdir_jpg(IMAGES_FILEPATH)))\n",
    "\n",
    "print(\"Total number of images: {}\".format(number_of_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading processed images: 100%|██████████| 10000/10000 [00:38<00:00, 262.06it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_images():\n",
    "    \"\"\"Load and return standard size images.\"\"\"\n",
    "    should_preprocess = not (os.path.exists(PROCESSED_IMAGES_FILEPATH) \\\n",
    "        and len(list(listdir_jpg(PROCESSED_IMAGES_FILEPATH))) == number_of_images)\n",
    "    \n",
    "    image_dir = IMAGES_FILEPATH if should_preprocess else PROCESSED_IMAGES_FILEPATH\n",
    "    images_gen = sorted(listdir_jpg(image_dir))\n",
    "    tqdm_desc = \"Loading {} images\".format(\"raw\" if should_preprocess else \"processed\")\n",
    "    images = [cv2.imread(os.path.join(image_dir, img)) for img in tqdm(images_gen, desc=tqdm_desc)]\n",
    "    \n",
    "    \n",
    "    if should_preprocess:\n",
    "        if not os.path.exists(PROCESSED_IMAGES_FILEPATH):\n",
    "            os.makedirs(PROCESSED_IMAGES_FILEPATH)\n",
    "        pbar = tqdm(total=len(images), desc=\"Processing image size\", position=0)\n",
    "        for i, image in enumerate(images):\n",
    "            # Preprocess\n",
    "            image = cv2.resize(image, dsize=IMAGE_DIMS, interpolation=cv2.INTER_LANCZOS4)\n",
    "            images[i] = image\n",
    "            # Save for later\n",
    "            cv2.imwrite(os.path.join(PROCESSED_IMAGES_FILEPATH, \"{:05d}.jpg\".format(i)), image)\n",
    "            pbar.update()\n",
    "    return images\n",
    "\n",
    "images = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize images to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing images: 100%|██████████| 10000/10000 [00:51<00:00, 196.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(310, 460, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "images_normalized = [cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F) for image in tqdm(images, desc=\"Normalizing images\")]\n",
    "\n",
    "print(images_normalized[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TripletImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, triplet_labels_fp, images):\n",
    "        \"\"\"Create triplets from triplet_labels_fp\"\"\"\n",
    "        \n",
    "        self.images = images\n",
    "        \n",
    "        triplets = []\n",
    "        for line in tqdm(open(triplet_labels_fp, 'r')):\n",
    "            anchor, positive, negative = tuple(map(int, line.split()))\n",
    "            triplets.append((anchor, positive, negative))\n",
    "        \n",
    "        self.triplets = triplets\n",
    "        \n",
    "    # Required for Map-style dataset\n",
    "    def __getitem__(self, index):\n",
    "        i_1, i_2, i_3 = self.triplets[index]\n",
    "        return images[i_1], images[i_2], images[i_3]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def _img_to_tensor(self, img):\n",
    "        return img.float().permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    def generate_batches(self, batch_size, shuffle=True, drop_last=True):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
    "                                shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "        for img1, img2, img3 in dataloader:\n",
    "            yield self._img_to_tensor(img1), self._img_to_tensor(img2), self._img_to_tensor(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59515it [00:00, 184162.92it/s]\n",
      "59544it [00:00, 383931.68it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set = TripletImageDataset('data/train_triplets.txt', images_normalized)\n",
    "test_set = TripletImageDataset('data/test_triplets.txt', images_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    EMBEDDING_DIM=50\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 7),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 256, 1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(256, 28, 1),\n",
    "            nn.PReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(52416, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, NeuralNet.EMBEDDING_DIM)\n",
    "        )\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 52416)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, margin=0.5):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        return (x1 - x2).pow(2).sum(1)\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_pos = self.euclidean_distance(anchor, positive)\n",
    "        distance_neg = self.euclidean_distance(anchor, negative)\n",
    "        # Torch.relu(x) = max(0, x)\n",
    "        losses = torch.relu(distance_pos - distance_neg + self.margin)\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \n",
    "    def start(self, model, dataset, batch_size):\n",
    "        epochs = 5\n",
    "        nr_batches = len(dataset) // batch_size\n",
    "        progress_bar = tqdm(desc='', total=epochs * nr_batches,\n",
    "            leave=False, ncols=80\n",
    "        )\n",
    "        # Start the training\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = []\n",
    "            batches = dataset.generate_batches(batch_size)\n",
    "            for batch_index, (anchor_img, positive_img, negative_img) in enumerate(batches):\n",
    "                progress_bar.set_description_str(\"E {} | B {}\".format(\n",
    "                    epoch, batch_index\n",
    "                ))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                anchor_out = model(anchor_img)\n",
    "                positive_out = model(positive_img)\n",
    "                negative_out = model(negative_img)\n",
    "\n",
    "                loss = criterion(anchor_out, positive_out, negative_out)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_t = loss.cpu().detach().numpy()\n",
    "                running_loss.append(loss_t)\n",
    "                \n",
    "                progress_bar.set_postfix_str(\"Loss={0:.3f}\".format(loss_t))\n",
    "                progress_bar.update()\n",
    "                \n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), 'trained_models/model')\n",
    "            \n",
    "            progress_bar.write(\n",
    "                \"[Epoch {}/{}]: Loss {}\".format(\n",
    "                    epoch+1, epochs,  np.mean(running_loss)\n",
    "                )\n",
    "            )\n",
    "            progress_bar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|                                                 | 0/18595 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "E 0 | B 0:   0%|                                      | 0/18595 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "E 0 | B 0:   0%|                    | 0/18595 [00:24<?, ?it/s, Loss=1154116.500]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "E 0 | B 0:   0%|        | 1/18595 [00:24<124:29:33, 24.10s/it, Loss=1154116.500]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "E 0 | B 1:   0%|        | 1/18595 [00:24<124:29:33, 24.10s/it, Loss=1154116.500]\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e0e128ffa96a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-c7becb9cab7e>\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, model, dataset, batch_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchor_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iml/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/iml/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = TripletLoss()\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "Training().start(model, train_set, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "iml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
