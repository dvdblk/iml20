{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "10.2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Using device {}\".format(device))\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images / Data\n",
    "\n",
    "## Standardize\n",
    "\n",
    "* Change the size of each image to the median values of the entire image set (460 x 310)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images: 10000\n"
     ]
    }
   ],
   "source": [
    "IMAGES_FILEPATH = 'data/food'\n",
    "PROCESSED_IMAGES_FILEPATH = 'data/food_processed'\n",
    "IMAGE_DIMS = (460, 310)\n",
    "\n",
    "def listdir_jpg(path):\n",
    "    \"\"\"os.listdir but only for jpg\"\"\"\n",
    "    for f in os.listdir(path):\n",
    "        _, extension = os.path.splitext(f)\n",
    "        if extension == \".jpg\":\n",
    "            yield f\n",
    "\n",
    "number_of_images = len(list(listdir_jpg(IMAGES_FILEPATH)))\n",
    "\n",
    "print(\"Total number of images: {}\".format(number_of_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b6c623d7c24ffca64c73b09c7d02b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Loading processed images', max=10000.0, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def load_images():\n",
    "    \"\"\"Load and return standard size images.\"\"\"\n",
    "    should_preprocess = not (os.path.exists(PROCESSED_IMAGES_FILEPATH) \\\n",
    "        and len(list(listdir_jpg(PROCESSED_IMAGES_FILEPATH))) == number_of_images)\n",
    "    \n",
    "    image_dir = IMAGES_FILEPATH if should_preprocess else PROCESSED_IMAGES_FILEPATH\n",
    "    images_gen = sorted(listdir_jpg(image_dir))\n",
    "    tqdm_desc = \"Loading {} images\".format(\"raw\" if should_preprocess else \"processed\")\n",
    "    images = [cv2.imread(os.path.join(image_dir, img)) for img in tqdm(images_gen, desc=tqdm_desc)]\n",
    "    \n",
    "    \n",
    "    if should_preprocess:\n",
    "        if not os.path.exists(PROCESSED_IMAGES_FILEPATH):\n",
    "            os.makedirs(PROCESSED_IMAGES_FILEPATH)\n",
    "        pbar = tqdm(total=len(images), desc=\"Processing image size\", position=0)\n",
    "        for i, image in enumerate(images):\n",
    "            # Preprocess\n",
    "            image = cv2.resize(image, dsize=IMAGE_DIMS, interpolation=cv2.INTER_LANCZOS4)\n",
    "            images[i] = image\n",
    "            # Save for later\n",
    "            cv2.imwrite(os.path.join(PROCESSED_IMAGES_FILEPATH, \"{:05d}.jpg\".format(i)), image)\n",
    "            pbar.update()\n",
    "    return images\n",
    "\n",
    "images = load_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize images to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca86bfcdd4240a9a8a19d59ac35f5b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Normalizing images', max=10000.0, style=ProgressStyle(des…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, image in tqdm(enumerate(images), desc=\"Normalizing images\", total=len(images)):\n",
    "    images[i] = cv2.normalize(image, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TripletImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, triplet_labels_fp):\n",
    "        \"\"\"Create triplets from triplet_labels_fp\"\"\"\n",
    "        triplets = []\n",
    "        for line in open(triplet_labels_fp, 'r'):\n",
    "            anchor, positive, negative = tuple(map(int, line.split()))\n",
    "            triplets.append((anchor, positive, negative))\n",
    "        \n",
    "        self.triplets = triplets\n",
    "        \n",
    "    # Required for Map-style dataset\n",
    "    def __getitem__(self, index):\n",
    "        i_1, i_2, i_3 = self.triplets[index]\n",
    "        return images[i_1], images[i_2], images[i_3]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "    \n",
    "    def _img_to_tensor(self, img):\n",
    "        return img.float().permute(0, 3, 1, 2).to(device)\n",
    "    \n",
    "    def generate_batches(self, batch_size, shuffle=True, drop_last=True):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
    "                                shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "        for img1, img2, img3 in dataloader:\n",
    "            yield self._img_to_tensor(img1), self._img_to_tensor(img2), self._img_to_tensor(img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "855664d41a264788870a2395a3311204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = TripletImageDataset('data/train_triplets.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    \n",
    "    EMBEDDING_DIM=50\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 256, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.3),\n",
    "            nn.Conv2d(256, 28, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            #nn.Dropout(0.3),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3276, 512),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(512, NeuralNet.EMBEDDING_DIM)\n",
    "        )\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            torch.nn.init.kaiming_normal_(m.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \n",
    "    def start(self, model, dataset, batch_size, epochs=15, model_name='model.pth'):\n",
    "        nr_batches = len(dataset) // batch_size\n",
    "        progress_bar = tqdm(desc='', total=epochs * nr_batches,\n",
    "            leave=False\n",
    "        )\n",
    "        # Start the training\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = []\n",
    "            running_pred = []\n",
    "            batches = dataset.generate_batches(batch_size)\n",
    "            for batch_index, (anchor_img, positive_img, negative_img) in enumerate(batches):\n",
    "                progress_bar.set_description_str(\"E {} | B {}\".format(\n",
    "                    epoch+1, batch_index\n",
    "                ))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                anchor_out = model(anchor_img)\n",
    "                positive_out = model(positive_img)\n",
    "                negative_out = model(negative_img)\n",
    "\n",
    "                loss = criterion(anchor_out, positive_out, negative_out)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                loss_t = loss.cpu().detach().numpy()\n",
    "                running_loss.append(loss_t)\n",
    "                \n",
    "                predicted_labels = closest_image(anchor_out, positive_out, negative_out)\n",
    "                running_pred.append(predicted_labels.cpu().numpy())\n",
    "                \n",
    "                progress_bar.set_postfix_str(\"Loss={0:.3f}\".format(np.mean(running_loss)))\n",
    "                progress_bar.update()\n",
    "                \n",
    "            # Save the model\n",
    "            model_fp = 'trained_models/'\n",
    "            if not os.path.exists(model_fp):\n",
    "                os.makedirs(model_fp)\n",
    "            torch.save(model.state_dict(), os.path.join(model_fp, model_name))\n",
    "            # Compute accuracy\n",
    "            predictions = np.array(running_pred).flatten()\n",
    "            acc = accuracy(predictions, np.ones_like(predictions))\n",
    "            # Update progress bar\n",
    "            progress_bar.write(\n",
    "                \"[Epoch {}/{}]: Loss {}, Acc {}\".format(\n",
    "                    epoch+1, epochs,  np.mean(running_loss), acc\n",
    "                )\n",
    "            )\n",
    "            progress_bar.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed571e0a231b465bac53f5dbc14996d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=55785.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/15]: Loss 0.4536386728286743\n",
      "[Epoch 2/15]: Loss 0.4665737748146057\n",
      "[Epoch 3/15]: Loss 0.45581337809562683\n",
      "[Epoch 4/15]: Loss 0.4243156313896179\n",
      "[Epoch 5/15]: Loss 0.41445910930633545\n",
      "[Epoch 6/15]: Loss 0.4018169641494751\n",
      "[Epoch 7/15]: Loss 0.3813132643699646\n",
      "[Epoch 8/15]: Loss 0.3544401228427887\n",
      "[Epoch 9/15]: Loss 0.32084590196609497\n",
      "[Epoch 10/15]: Loss 0.28474193811416626\n",
      "[Epoch 11/15]: Loss 0.2514423131942749\n",
      "[Epoch 12/15]: Loss 0.219896599650383\n",
      "[Epoch 13/15]: Loss 0.19316422939300537\n",
      "[Epoch 14/15]: Loss 0.17123612761497498\n",
      "[Epoch 15/15]: Loss 0.1528848111629486\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet()\n",
    "model.zero_grad()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "Training().start(model, train_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = TripletImageDataset('data/test_triplets.txt')\n",
    "\n",
    "def closest_image(anchor, positive, negative):\n",
    "    \"\"\"\n",
    "    Returns: \n",
    "        int: 1 if the anchor is more similar to the positive image. \n",
    "                0 if anchor is more similar to the negative image.\n",
    "                \n",
    "    Note:\n",
    "        Works for batches. \n",
    "        Each input tensor should have this shape: `N x W x H x C` \n",
    "        where `N` is the number of samples.\n",
    "    \"\"\"\n",
    "    pairwise_dist = nn.PairwiseDistance(p=2)\n",
    "    distance_pos = pairwise_dist(anchor, positive)\n",
    "    distance_neg = pairwise_dist(anchor, negative)\n",
    "    return (~(distance_pos >= distance_neg).to(torch.bool)).to(torch.int32)\n",
    "\n",
    "def predict(predict_set):\n",
    "    \"\"\"Predict 0/1 for the given predict_set (Dataset)\"\"\"\n",
    "    predict_set_choices = []\n",
    "    # List of tuples which the model chose as most similar\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for anchor_img, positive_img, negative_img in tqdm(predict_set.generate_batches(batch_size, shuffle=False, drop_last=False), total=len(predict_set) // batch_size):\n",
    "            # Get embeddings from our model\n",
    "            anchor_emb = model(anchor_img)\n",
    "            positive_emb = model(positive_img)\n",
    "            negative_emb = model(negative_img)\n",
    "\n",
    "            # Compute distances and the corresponding labels\n",
    "            labels = closest_image(anchor_emb, positive_emb, negative_emb)\n",
    "\n",
    "            predict_set_choices.append(labels.cpu().numpy())\n",
    "    result = np.array(predict_set_choices).flatten()\n",
    "    return np.concatenate(result).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0348dc883d2f42e18322b15ef3b2dd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3719.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set_predictions = predict(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy: 0.9456607577921532\n"
     ]
    }
   ],
   "source": [
    "def accuracy(pred, gold):\n",
    "    return np.mean((pred == gold))\n",
    "    \n",
    "print(\"Train set accuracy: {}\".format(accuracy(train_set_predictions, np.ones_like(train_set_predictions))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca5c508ee058421a8a5126f613c483e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3721.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "last_test_set_predictions = test_set_predictions\n",
    "test_set_predictions = predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59544 (59544,)\n",
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "print(len(test_set_predictions), test_set_predictions.shape)\n",
    "print(type(test_set_predictions[0]))\n",
    "np.savetxt('data/submission.txt', test_set_predictions, fmt=\"%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "print(test_set_choices)\n",
    "def show_predicted_images():\n",
    "    fig = plt.figure(figsize=(20, 40))\n",
    "    n_images_to_show = 5\n",
    "    images = next(test_set.generate_batches(n_images_to_show, shuffle=False))\n",
    "    gs = gridspec.GridSpec(n_images_to_show, 3)\n",
    "    for i in range(n_images_to_show):\n",
    "        for j in range(len(images)):\n",
    "            ax = fig.add_subplot(gs[i, j])\n",
    "            if (j == 2 - test_set_choices[i]):\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor('green')\n",
    "                    spine.set_linewidth('7')\n",
    "            else:\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_edgecolor('green')\n",
    "                    spine.set_linewidth('0')\n",
    "            ax.imshow(images[j][i].cpu().permute(1, 2, 0))\n",
    "            \n",
    "            ax.tick_params(axis='both', which='both', bottom=False, top=False, labelbottom=False, left=False, labelleft=False)\n",
    "    plt.show()\n",
    "    \n",
    "show_predicted_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31bb25a5b0c4617a49bc9e336fb6eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=33471.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/9]: Loss 0.1367347687482834, Acc 0.8920744823877387\n",
      "[Epoch 2/9]: Loss 0.12115588784217834, Acc 0.9058046517881151\n",
      "[Epoch 3/9]: Loss 0.10844552516937256, Acc 0.9166106480236623\n",
      "[Epoch 4/9]: Loss 0.09669335931539536, Acc 0.9263746974993278\n",
      "[Epoch 5/9]: Loss 0.08912786096334457, Acc 0.9318029040064534\n",
      "[Epoch 6/9]: Loss 0.08189882338047028, Acc 0.9374663888141974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-a25d6334b072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mTraining\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'retrained_model.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-59-bc5a380f74d8>\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self, model, dataset, batch_size, epochs, model_name)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mrunning_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0manchor_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpositive_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnegative_img\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 progress_bar.set_description_str(\"E {} | B {}\".format(\n\u001b[0;32m     16\u001b[0m                     \u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-18f60d9e2186>\u001b[0m in \u001b[0;36mgenerate_batches\u001b[1;34m(self, batch_size, shuffle, drop_last)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_img_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_img_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_img_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-18f60d9e2186>\u001b[0m in \u001b[0;36m_img_to_tensor\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_img_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('trained_models/model.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.TripletMarginLoss(margin=0.5)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "Training().start(model, train_set, batch_size, epochs=9, model_name='retrained_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "iml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
